{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3176,
     "status": "ok",
     "timestamp": 1556958625811,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "uBT9A7nmFQJ4",
    "outputId": "cc261b3e-0817-465b-e33e-9070253ed5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#mount data code here add the path to datapath variable\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1556958628278,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "JZiTFlT2TEYO",
    "outputId": "9b345065-3336-449a-a451-75014c45b7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " buildgraph.ipynb\t\t      'Project Report.gdoc'\n",
      " Capstone_Video.mp4\t\t       questions.txt\n",
      " KnowledgeGraph_presentation.gslides   README.txt.gdoc\n",
      " KnowledgeGraph_presentation.pdf      'Team Member Contributions.gdoc'\n",
      " movie_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/content/gdrive/My Drive/Capstone Project/'\n",
    "!ls '/content/gdrive/My Drive/Capstone Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqUbWD8jS0R3"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import json\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1556958632622,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "4qhhCi3vxU2_",
    "outputId": "763289b7-dc9d-4c32-9add-8808ef271739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luHrGV_4TWiJ"
   },
   "outputs": [],
   "source": [
    "G=nx.Graph() #Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q391BxelR75_"
   },
   "outputs": [],
   "source": [
    "row_index=0\n",
    "counter=0 #used to assign a unique node ID to each node.\n",
    "with open(DATA_PATH+'movie_metadata.csv') as csv_file:\n",
    "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "  #each of the below dictionaries are done in order to maintain state. For example, every actor \"gets\" at most one node in the graph.\n",
    "  #When we see the same actor occur again in a dataset, we muct not create another node for them, as the node already exists.\n",
    "  actors={}\n",
    "  directors={}\n",
    "  genres={}\n",
    "  titles={}\n",
    "  languages={}\n",
    "  for row in csv_reader: #iterates through each row of the dataset\n",
    "    \n",
    "    row=[re.sub(\"[:!',.&]\",\"\",s) for s in row]\n",
    "    if row_index!=0: #In order to leave out the header\n",
    "      \n",
    "      if row[4] not in titles: #checks if the node is already present ; if so, a new one need not be created.\n",
    "        counter+=1\n",
    "        G.add_node(counter,title=row[4]) #adding the node to the graph. The node is of type \"title\"; the title is selected from the current row of the dataset and it's index is given by the variable counter.\n",
    "        titles[row[4]]=counter #adding it to the dictionary. The dictionary contains title:nodenumber key value pairs\n",
    "      \n",
    "      if row[0] not in directors:\n",
    "        counter+=1\n",
    "        G.add_node(counter,director=row[0])\n",
    "        directors[row[0]]=counter\n",
    "      \n",
    "      if row[1] not in actors:\n",
    "        counter+=1\n",
    "        G.add_node(counter,actor=row[1])\n",
    "        actors[row[1]]=counter\n",
    "      \n",
    "      genres_list=row[2].split(\"|\") #genres are listed as so- Action|Adventure|Thriller; we split them up into a list\n",
    "      \n",
    "      for genre in genres_list:\n",
    "        if genre not in genres:\n",
    "          counter+=1\n",
    "          G.add_node(counter,genre=genre)\n",
    "          genres[genre]=counter\n",
    "        \n",
    "      if row[3] not in actors:\n",
    "        counter+=1\n",
    "        G.add_node(counter,actor=row[3])\n",
    "        actors[row[3]]=counter\n",
    "      \n",
    "      if row[5] not in actors:\n",
    "        counter+=1\n",
    "        G.add_node(counter,actor=row[5])\n",
    "        actors[row[5]]=counter\n",
    "      \n",
    "      if row[6] not in languages:\n",
    "        counter+=1\n",
    "        G.add_node(counter,language=row[6])\n",
    "        languages[row[6]]=counter\n",
    "        \n",
    "      \n",
    "      G.add_edge(directors[row[0]],titles[row[4]],relation=\"directs\") #adding an attributed edge between the movie and director\n",
    "      G.add_edge(actors[row[1]],titles[row[4]],relation=\"acts\")\n",
    "      G.add_edge(actors[row[3]],titles[row[4]],relation=\"acts\")\n",
    "      G.add_edge(actors[row[5]],titles[row[4]],relation=\"acts\")\n",
    "      for genre in genres_list:\n",
    "          G.add_edge(genres[genre],titles[row[4]],relation=\"genre\")\n",
    "      G.add_edge(languages[row[6]],titles[row[4]],relation=\"language\")\n",
    "            \n",
    "    row_index+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-h7UnoCd-gH"
   },
   "outputs": [],
   "source": [
    "#The below code draws the graph. Since we have nodes in the scale of thousands, this takes a lot of time and hence we do not run it.\n",
    "#pos = nx.spring_layout(G,scale=4)\n",
    "#nx.draw(G, with_labels=True, node_size=200,font_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHFBREy4XUD5"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/48660547/how-can-i-extract-gpelocation-using-nltk-ne-chunk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_continuous_chunks(text, label):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQd4r2AUlDus"
   },
   "outputs": [],
   "source": [
    "#We parse questions from our question dataset for entities and relations. We query the above generated graph with the parsed entities and relation \n",
    "#and compare it to the results obtained by querying the Google Knowledge Graph API.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def fn_preprocess(art):\n",
    "    art = nltk.word_tokenize(art) #The sentence is converted into tokens \n",
    "    art = nltk.pos_tag(art) #Each of the tokens are tagged with their appropriate parts of speech\n",
    "    return art\n",
    "  \n",
    "\n",
    "#Take the token that is a verb(acts,directs,stars, etc) and convert it to its stemmed form. This will be the relation part of the query to the \n",
    "#Entity relation graph.\n",
    "\n",
    "def create_entity_tuple(query):\n",
    "  query_processed = fn_preprocess(query)\n",
    "  NE_list=get_continuous_chunks(query,'PERSON')\n",
    "  #print(NE_list)\n",
    "  results = ne_chunk(query_processed)\n",
    "  #cp = nltk.RegexpParser(pattern)\n",
    "  cs = results #cp.parse(query_processed)\n",
    "\n",
    "  #print(cs)\n",
    "\n",
    "  #Below are dictionaries of words and synonyms that we are concerned with with respect to the relations in our graph such as acts,directs\n",
    "  function_mapper={'director':'title','actor':'title','genre':'title', 'language':'title'}\n",
    "  acts={'star','act','play','actor','lead','appear'}\n",
    "  directs={'direct','director'}\n",
    "  genres={'genre','type','genr','typ'}\n",
    "  languages={'language','languag'}\n",
    "  nltk_names={'PRP','PRP$','VB','VBD','VBG','VBN','VBP','NN','NNS','NNP','NPS','DT'}\n",
    "  iob_tagged = tree2conlltags(cs) #IOB tagging the words\n",
    "\n",
    "  relation_token_seen=0\n",
    "  next_token_seen=0\n",
    "  rest_of_sentence=[]\n",
    "  NNP_list=[]\n",
    "  nnp_token_seen=0\n",
    "  \n",
    "  for x in iob_tagged:\n",
    "    stemmed_x=stemmer.stem(x[0]) #We get the stemmed version of the word making it compatible with the dictionary eg: acted,acting->act\n",
    "    #print(stemmed_x)\n",
    "    if stemmed_x in acts: #Looking in each dictionary\n",
    "      relation=\"actor\"\n",
    "      relation_token_seen=1\n",
    "    \n",
    "    elif stemmed_x in directs:\n",
    "      relation_token_seen=1\n",
    "      relation=\"director\"\n",
    "      \n",
    "    elif stemmed_x in genres:\n",
    "      relation_token_seen=1\n",
    "      relation=\"genre\"\n",
    "      \n",
    "    elif stemmed_x in languages:\n",
    "      relation_token_seen=1\n",
    "      relation=\"language\"\n",
    "      \n",
    "    elif(relation_token_seen==1): #This token indicates that the required relation has already been seen in the string.\n",
    "      if(x[1] in nltk_names):\n",
    "        rest_of_sentence.append(x[0])\n",
    "        next_token_seen=1\n",
    "        relation_token_seen=0\n",
    "      \n",
    "    if(next_token_seen==1):\n",
    "      rest_of_sentence.append(x[0])\n",
    "      \n",
    "    \n",
    "    entity_name=\" \".join(rest_of_sentence[1:-1])\n",
    "  \n",
    "  if relation in ['actor','director'] and NE_list and len(NE_list)<=2:\n",
    "    #print(relation,NE_list[0])\n",
    "    entity_name=NE_list[0]\n",
    "    if relation=='actor':\n",
    "      function_map='actor'\n",
    "      relation='title'\n",
    "    elif relation=='director':\n",
    "      function_map='director'\n",
    "      relation='title'\n",
    "  else:\n",
    "    function_map=function_mapper[relation]\n",
    "      \n",
    "    \n",
    "  return(function_map,entity_name.lower(),relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ttf2vMBGKtr"
   },
   "outputs": [],
   "source": [
    "def get_entity_relation_from_graph(entity,entity_name,relation): \n",
    "  #In a question such as \"Who directed Avengers?\"; \n",
    "  #we expect the extracted parameters to be entity:title,entity_name:Avengers;relation:direct\n",
    "  \n",
    "  node_list=list(G.nodes(data=entity)) #In our example, this gets all the \"title\" attributed nodes.\n",
    "  \n",
    "  match_list=[] \n",
    "  \n",
    "  for node in node_list:\n",
    "    if node[1]==entity_name: #Looks for a match for \"Avengers\"\n",
    "      match_list.append(node[0])\n",
    "   \n",
    "  answer=[]\n",
    "  for matched_node in match_list:\n",
    "    for entity_node in G.edges(matched_node): #Look for all outgoing edges; follow he edge with the attribute(relation) that we need\n",
    "      if relation in G.nodes[entity_node[1]]: \n",
    "        answer.append(G.nodes[entity_node[1]][relation])\n",
    "      \n",
    "  return answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1556958667721,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "wo_6hqx5AdCf",
    "outputId": "1224b62d-66bd-45e0-d8a5-f72dfc4aabd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('title', 'the avengers', 'director')\n",
      "['joss whedon']\n"
     ]
    }
   ],
   "source": [
    "#Function example: You can look for how the function creates the entity tuple, and how it obtains the response for a question\n",
    "#of choice using the below code snippet. We make certain assumptions.\n",
    "#1)We handle only binary questions: Questions such as \"Who directed I am Legend\" rather than \"Who directs and acts in I am legend\"\n",
    "#2)We do not handle questions in passive voice: Questions such as \"I am legend was directed by whom?\"\n",
    "#3)We do not handle cases with movies just named with the entities, such as, \"Who directed Robin Hood?\"\n",
    "#4)We do not hande very vague terms to inference a relation, such as, \"Who is IN I am legend?\"\n",
    "\n",
    "entity_tuple=create_entity_tuple(\"Who directed The Avengers?\")\n",
    "print(entity_tuple)\n",
    "answer_list=get_entity_relation_from_graph(entity_tuple[0],entity_tuple[1],entity_tuple[2])\n",
    "print(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12127,
     "status": "ok",
     "timestamp": 1556958680961,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "OtQYVYZpS3-m",
    "outputId": "a01b3ef8-fc90-4e38-883a-54046226c4bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct/Overall: 158/262\n",
      "Accuracy: 60.3053435115\n"
     ]
    }
   ],
   "source": [
    "#We open the dataset and read line by line. We send this information to the function and collect the outputs. This can then be compared\n",
    "#to the equivalent results from Google's Knowledge Graph API. We calculate accuracy in terms of the number of questions we get a non-empty response\n",
    "#from the function.\n",
    "\n",
    "with open(DATA_PATH+'questions.txt') as file:\n",
    "  questions=file.readlines()\n",
    "  query_counter=0\n",
    "  right_counter=0\n",
    "  for question in questions:\n",
    "    query_counter+=1\n",
    "    question=question.decode(\"utf-8\")\n",
    "    question=re.sub(\"[:!',.&]\",\"\",question)\n",
    "    entity_tuple=create_entity_tuple(question)\n",
    "    answer_list=get_entity_relation_from_graph(entity_tuple[0],entity_tuple[1],entity_tuple[2])\n",
    "    if(answer_list):\n",
    "        right_counter+=1\n",
    "  print(\"Correct/Overall: \"+str(right_counter)+\"/\"+str(query_counter))\n",
    "  print(\"Accuracy: \"+str(((1.0*right_counter)/(1.0*query_counter))*100))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foYyJ6vIWP2s"
   },
   "outputs": [],
   "source": [
    "#API key given  below is our acquired key.\n",
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1556958760547,
     "user": {
      "displayName": "Nandita Srinivasan",
      "photoUrl": "",
      "userId": "11793666876757011364"
     },
     "user_tz": 240
    },
    "id": "uWC0M_c7TGat",
    "outputId": "f4b0c631-41a2-410e-dcc6-fe1834d6404e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'I Am Legend', '--', {u'url': u'https://en.wikipedia.org/wiki/I_Am_Legend_(film)', u'articleBody': u'I Am Legend is a 2007 American post-apocalyptic science fiction horror film based on the novel of the same name, directed by Francis Lawrence and starring Will Smith, who plays US Army virologist Robert Neville. ', u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License'})\n",
      "(u'I Am Legend', '--', {u'url': u'https://en.wikipedia.org/wiki/I_Am_Legend_(novel)', u'articleBody': u'I Am Legend is a 1954 science fiction horror novel by American writer Richard Matheson. It was influential in the development of the zombie-vampire genre and in popularizing the concept of a worldwide apocalypse due to disease. ', u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License'})\n",
      "(u'I Am Legend', '--', {u'url': u'https://en.wikipedia.org/wiki/I_Am_Legend_(TV_series)', u'articleBody': u'I Am Legend is a South Korean television series starring Kim Jung-eun that was broadcast on SBS on August 2 to September 21, 2010.', u'license': u'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License'})\n"
     ]
    }
   ],
   "source": [
    "    api_key = API_KEY\n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "\n",
    "    query = 'i am legend'\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': 4,\n",
    "        'indent': True,\n",
    "        'key': api_key,\n",
    "    }\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    response = json.loads(urllib.urlopen(url).read())\n",
    "    mov_details=[]\n",
    "    names=[]\n",
    "    for element in response['itemListElement']:\n",
    "      if 'detailedDescription' in element['result'] and 'name' in element['result']:\n",
    "          print (element['result']['name'],\"--\", element['result']['detailedDescription'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "buildgraph.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
